{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Neccessary Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'grequests'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-839c419f5704>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgrequests\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mareq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mselenium\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbs4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'grequests'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import grequests as areq\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Neccessary Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "entities_to_search = [\"First Abu Dhabi Bank\", \n",
    "                      \"NBAD\", \n",
    "                      \"FAB\", \n",
    "                      \"Bank of Abu Dhabi First\", \n",
    "                      \"بنك أبوظبي الأول\", \n",
    "                      \"Abu Dhabi First\", \n",
    "                      \"و أ ب\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawlers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modularized Crawlers For Each Outlet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reuters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def reuters_crawler(query_link: str, path_to_driver=driver_path):\n",
    "    \"\"\"\n",
    "    The purpose of this function is to provide the user with a function\n",
    "    that allows them to obtain several (if not all) of the articles that\n",
    "    are returned from a query done on the Reuters search feature that \n",
    "    mention the company (which can be mentioned by any of its aliases)\n",
    "    in its text.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    query_link : str\n",
    "        This string allows the user to specify the URL of the search query\n",
    "        that they performed for this outlet.\n",
    "    path_to_driver : str\n",
    "        This string allows the user to specify where on their local machine\n",
    "        a Google driver exec object can be found.\n",
    "        \n",
    "        NOTE that its default value is \"../Data/chromedriver\"\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    to_return : list of dict objects\n",
    "        This is a list of dictionaries where each dictionary corresponds\n",
    "        to an article that was found in the search query. Each dictionary\n",
    "        contains the information that was retrieved for each article.\n",
    "    \n",
    "    References\n",
    "    ----------\n",
    "    1. https://stackoverflow.com/questions/20986631/how-can-i-scroll-a-web-page-using-selenium-webdriver-in-python\n",
    "    2. https://pypi.org/project/beautifulsoup4/\n",
    "    3. https://github.com/spyoungtech/grequests\n",
    "    4. https://realpython.com/modern-web-automation-with-python-and-selenium/\n",
    "    5. https://stackoverflow.com/questions/11205386/python-beautifulsoup-get-an-attribute-value-based-on-the-name-attribute\n",
    "    6. https://docs.python.org/3.3/library/datetime.html\n",
    "    7. https://www.educative.io/edpresso/how-to-convert-a-string-to-a-date-in-python\n",
    "    8. https://www.dataquest.io/blog/web-scraping-tutorial-python/\n",
    "    \"\"\"\n",
    "    # First, collect necccessary parameters that will be used for the rest\n",
    "    # of the function and verify the inputed data.\n",
    "    assert isinstance(query_link, str)\n",
    "    assert \"reuters\" in query_link\n",
    "    \n",
    "    # Now instantiate the driver and use it to navigate to the page. Then,\n",
    "    # scroll down a few times so that we can have access to more articles.\n",
    "    driver_to_use = webdriver.Chrome(path_to_driver)\n",
    "    driver_to_use.get(query_link)\n",
    "    \n",
    "    num_times_scroll_down = 33 \n",
    "        # As of 9/3/20, this is the number of scrolls that results in no\n",
    "        # more articles being loaded.\n",
    "    num_seconds_between_scrolls = 0.5\n",
    "    time.sleep(num_seconds_between_scrolls*3)\n",
    "    for i in range(num_times_scroll_down):\n",
    "        # Iterate for the number of times that is specifed above.\n",
    "        driver_to_use.execute_script(\n",
    "            \"window.scrollTo(0, document.body.scrollHeight);\"\n",
    "        )\n",
    "        time.sleep(num_seconds_between_scrolls)\n",
    "        \n",
    "    # Now, collect all of the article links that make up this page.\n",
    "    a_elements_list = driver_to_use.find_elements_by_tag_name(\"a\")\n",
    "    article_a_elements = [\n",
    "        a for a in a_elements_list \\\n",
    "        if a.get_attribute(\"class\") == \\\n",
    "        \"TextLabel__text-label___3oCVw TextLabel__black-to-orange___23uc0 TextLabel__medium___t9PWg MarketStoryItem-headline-2cgfz\"\n",
    "    ]\n",
    "    article_links_list = [\n",
    "        art.get_attribute(\"href\") for art in article_a_elements\n",
    "    ]\n",
    "    \n",
    "    # Close the driver since we will now longer be using it.\n",
    "    driver_to_use.close()\n",
    "    print(\n",
    "        \"Found {} Reuters articles from this search query.\".format(\n",
    "            len(article_links_list)\n",
    "        )\n",
    "    )\n",
    "    print(\"Now obtaining data for those articles.\")\n",
    "    \n",
    "    # Next, send requests to each of those links. Save the desired \n",
    "    # information for each article that was published within the last year\n",
    "    # and does in fact mention the company we're interested in (the\n",
    "    # aliases of which are listed above).\n",
    "    page_gets = (areq.get(url) for url in article_links_list)\n",
    "    page_gets_results = areq.map(page_gets)\n",
    "    \n",
    "    today_datetime = datetime.today()\n",
    "    page_dicts_list = []\n",
    "    for page in page_gets_results:\n",
    "        # Iterate over each of the page request results.\n",
    "        if page.ok:\n",
    "            # If the response to the article URL was successful.\n",
    "            page_contents = page.content\n",
    "            page_soup = BeautifulSoup(page_contents)\n",
    "            \n",
    "            # Get and verify published date.\n",
    "            article_pub_date = page_soup.find(\n",
    "                \"meta\", {\"name\": \"analyticsAttributes.articleDate\"}\n",
    "            )[\"content\"]\n",
    "            article_pub_datetime = datetime.strptime(\n",
    "                article_pub_date[:10:], \"%Y-%m-%d\"\n",
    "            )\n",
    "            days_since_published = (\n",
    "                today_datetime - article_pub_datetime\n",
    "            ).days\n",
    "            if days_since_published > 365:\n",
    "                # If the article was published beyond the past year.\n",
    "                continue\n",
    "            \n",
    "            # Get and verify article text (content).\n",
    "            content_html = page_soup.find(\n",
    "                \"div\", class_=\"StandardArticleBody_body\"\n",
    "            )\n",
    "            article_content = \"\".join(\n",
    "                [p_element.text for p_element in content_html.find_all(\"p\")\n",
    "            ])\n",
    "            mentions_an_alias = any([\n",
    "                alias.lower() in article_content.lower() \\\n",
    "                for alias in entities_to_search\n",
    "            ])\n",
    "            if not mentions_an_alias:\n",
    "                # If the article does NOT mention any of the aliases that\n",
    "                # the company of interest goes by in its text.\n",
    "                continue\n",
    "            \n",
    "            # Get article title.\n",
    "            article_title = page_soup.find(\n",
    "                \"meta\", property=\"og:title\"\n",
    "            )[\"content\"]\n",
    "            \n",
    "            # Get article URL.\n",
    "            article_url = page_soup.find(\n",
    "                \"meta\", {\"name\": \"analyticsAttributes.canonicalUrl\"}\n",
    "            )[\"content\"]\n",
    "            \n",
    "            # Get article author.\n",
    "            article_author = page_soup.find(\n",
    "                \"meta\", {\"name\": \"analyticsAttributes.author\"}\n",
    "            )[\"content\"]\n",
    "            \n",
    "            # Validate obtained data.\n",
    "            assert isinstance(article_title, str)\n",
    "            assert isinstance(article_url, str)\n",
    "            assert isinstance(article_author, str)\n",
    "            assert isinstance(article_pub_date, str)\n",
    "            assert isinstance(article_content, str)\n",
    "            \n",
    "            # Save obtained and verified data.\n",
    "            page_dict = {\"Title\": article_title,\n",
    "                         \"Article_URL\": article_url,\n",
    "                         \"Author\": article_author,\n",
    "                         \"Published_Date\": article_pub_date,\n",
    "                         \"Outlet\": \"reuters\",\n",
    "                         \"Content\": article_content}\n",
    "            page_dicts_list.append(page_dict)\n",
    "    \n",
    "    # Return the final results.\n",
    "    to_return = page_dicts_list\n",
    "\n",
    "    return to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "hello = reuters_crawler(\"https://www.reuters.com/companies/FAB.AD/news\")[:5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "example_dict = hello[3]\n",
    "example_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mediain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def mediain_crawler(query_link: str, path_to_driver=driver_path):\n",
    "    \"\"\"\n",
    "    The purpose of this function is to provide the user with a function\n",
    "    that allows them to obtain several (if not all) of the articles that\n",
    "    are returned from a query done on the Mediain search feature that \n",
    "    mention the company (which can be mentioned by any of its aliases)\n",
    "    in its text.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    query_link : str\n",
    "        This string allows the user to specify the URL of the search query\n",
    "        that they performed for this outlet.\n",
    "    path_to_driver : str\n",
    "        This string allows the user to specify where on their local machine\n",
    "        a Google driver exec object can be found.\n",
    "        \n",
    "        NOTE that its default value is \"../Data/chromedriver\"\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    to_return : list of dict objects\n",
    "        This is a list of dictionaries where each dictionary corresponds\n",
    "        to an article that was found in the search query. Each dictionary\n",
    "        contains the information that was retrieved for each article.\n",
    "    \n",
    "    References\n",
    "    ----------\n",
    "    1. https://stackoverflow.com/questions/56101612/python-requests-http-response-406\n",
    "    \"\"\"\n",
    "    # First, validate the inputted data.\n",
    "    assert isinstance(query_link, str)\n",
    "    assert \"mediain\" in query_link\n",
    "    \n",
    "    # Now, instantiate a BeautifulSoup object for the specified link and\n",
    "    # use it to navigate to the page. Then, compile a list of links for\n",
    "    # each page of the query.\n",
    "    query_req_setup = [areq.get(query_link, headers={\"User-Agent\": \"XY\"})]\n",
    "    query_req_obj = areq.map(query_req_setup)[0]\n",
    "    assert query_req_obj.ok\n",
    "    query_soup = BeautifulSoup(query_req_obj.content)\n",
    "    \n",
    "    number_of_pages = int(query_soup.find(\"span\", class_=\"pages\").text[-2::])\n",
    "    dot_com_index = query_link.index(\".com\") + len(\".com\")\n",
    "    query_page_links = [\n",
    "        \"{}{}{}\".format(query_link[:dot_com_index+1:], \n",
    "                        \"page/{}\".format(i), \n",
    "                        query_link[dot_com_index::]) \\\n",
    "        for i in range(1, number_of_pages+1)\n",
    "    ]\n",
    "        \n",
    "    # Next, collect all of the article links that make up these pages.\n",
    "    search_page_gets = (\n",
    "        areq.get(url, headers={\"User-Agent\": \"XY\"}) for url in query_page_links\n",
    "    )\n",
    "    search_page_gets_results = areq.map(search_page_gets)\n",
    "    assert all([page_get.ok for page_get in search_page_gets_results])\n",
    "    \n",
    "    article_links_list = []\n",
    "    for search_page in search_page_gets_results:\n",
    "        search_page_soup = BeautifulSoup(search_page.content)\n",
    "        links_list_search = search_page_soup.find_all(\n",
    "            \"article\", class_=\"item-list\"\n",
    "        )\n",
    "        page_article_links_list = [\n",
    "            link_soup.find(\"a\")[\"href\"] for link_soup in links_list_search\n",
    "        ]\n",
    "        article_links_list += page_article_links_list\n",
    "    assert len(article_links_list) <= 10*number_of_pages\n",
    "    print(\n",
    "        \"Found {} articles from this search query.\".format(\n",
    "            len(article_links_list)\n",
    "        )\n",
    "    )\n",
    "    print(\"Now obtaining data for those articles.\")\n",
    "    \n",
    "    # Next, send requests to each of those links. Save the desired \n",
    "    # information for each article that was published within the last year\n",
    "    # and does in fact mention the company we're interested in (the\n",
    "    # aliases of which are listed above).\n",
    "    page_gets = (\n",
    "        areq.get(url, headers={\"User-Agent\": \"XY\"}) for url in article_links_list\n",
    "    )\n",
    "    page_gets_results = areq.map(page_gets)\n",
    "    \n",
    "    today_datetime = datetime.today()\n",
    "    page_dicts_list = []\n",
    "    for page in page_gets_results:\n",
    "        # Iterate over each of the page request results.\n",
    "        if page.ok:\n",
    "            # If the response to the article URL was successful.\n",
    "            page_contents = page.content\n",
    "            page_soup = BeautifulSoup(page_contents)\n",
    "            \n",
    "            # Get and verify published date.\n",
    "            article_pub_date = page_soup.find(\n",
    "                \"meta\", property=\"article:published_time\"\n",
    "            )[\"content\"]\n",
    "            article_pub_datetime = datetime.strptime(\n",
    "                article_pub_date[:10:], \"%Y-%m-%d\"\n",
    "            )\n",
    "            days_since_published = (\n",
    "                today_datetime - article_pub_datetime\n",
    "            ).days\n",
    "            if days_since_published > 365:\n",
    "                # If the article was published beyond the past year.\n",
    "                continue\n",
    "            \n",
    "            # Get and verify article text (content).\n",
    "            content_html = page_soup.find(\n",
    "                \"div\", class_=\"entry\"\n",
    "            )\n",
    "            article_content = \"\".join(\n",
    "                [p_element.text for p_element in content_html.find_all(\"p\")\n",
    "            ])\n",
    "            mentions_an_alias = any([\n",
    "                alias.lower() in article_content.lower() \\\n",
    "                for alias in entities_to_search\n",
    "            ])\n",
    "            if not mentions_an_alias:\n",
    "                # If the article does NOT mention any of the aliases that\n",
    "                # the company of interest goes by in its text.\n",
    "                continue\n",
    "            \n",
    "            # Get article title.\n",
    "            article_title = page_soup.find(\n",
    "                \"meta\", property=\"og:title\"\n",
    "            )[\"content\"]\n",
    "            \n",
    "            # Get article URL.\n",
    "            article_url = page_soup.find(\n",
    "                \"meta\", property=\"og:url\"\n",
    "            )[\"content\"]\n",
    "            \n",
    "            # Get article description\n",
    "            article_description = page_soup.find(\n",
    "                \"meta\", property=\"og:description\"\n",
    "            )[\"content\"]\n",
    "            \n",
    "            # Validate obtained data.\n",
    "            assert isinstance(article_title, str)\n",
    "            assert isinstance(article_url, str)\n",
    "            assert isinstance(article_pub_date, str)\n",
    "            assert isinstance(article_content, str)\n",
    "            assert isinstance(article_description, str)\n",
    "            \n",
    "            # Save obtained and verified data.\n",
    "            page_dict = {\"Title\": article_title,\n",
    "                         \"Article_URL\": article_url,\n",
    "                         \"Published_Date\": article_pub_date,\n",
    "                         \"Outlet\": \"mediain\",\n",
    "                         \"Description\": article_description,\n",
    "                         \"Content\": article_content}\n",
    "            page_dicts_list.append(page_dict)\n",
    "    \n",
    "    # Return the final results.\n",
    "    to_return = page_dicts_list\n",
    "\n",
    "    return to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "mediain_crawler(\n",
    "    \"https://mediain.com/?s=%D8%A8%D9%86%D9%83+%D8%A3%D8%A8%D9%88%D8%B8%D8%A8%D9%8A+%D8%A7%D9%84%D8%A3%D9%88%D9%84&lang=en\"\n",
    ")[:5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "mediain_crawler(\n",
    "    \"https://mediain.com/?s=first+abu+dhabi+bank&lang=en\"\n",
    ")[:3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def ol_crawler(query_link: str, path_to_driver=driver_path):\n",
    "    \"\"\"\n",
    "    The purpose of this function is to provide the user with a function\n",
    "    that allows them to obtain several (if not all) of the articles that\n",
    "    are returned from a query done on the Ol search feature that \n",
    "    mention the company (which can be mentioned by any of its aliases)\n",
    "    in its text.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    query_link : str\n",
    "        This string allows the user to specify the URL of the search query\n",
    "        that they performed for this outlet.\n",
    "    path_to_driver : str\n",
    "        This string allows the user to specify where on their local machine\n",
    "        a Google driver exec object can be found.\n",
    "        \n",
    "        NOTE that its default value is \"../Data/chromedriver\"\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    to_return : list of dict objects\n",
    "        This is a list of dictionaries where each dictionary corresponds\n",
    "        to an article that was found in the search query. Each dictionary\n",
    "        contains the information that was retrieved for each article.\n",
    "    \n",
    "    References\n",
    "    ----------\n",
    "    1.\n",
    "    \"\"\"\n",
    "    # First, validate the inputted data.\n",
    "    assert isinstance(query_link, str)\n",
    "    assert \"ol\" in query_link\n",
    "    \n",
    "    # Now, instantiate a BeautifulSoup object for the specified link and\n",
    "    # use it to navigate to the page. Then, compile a list of links for\n",
    "    # each page of the query.\n",
    "    query_req_setup = [areq.get(query_link)]\n",
    "    query_req_obj = areq.map(query_req_setup)[0]\n",
    "    assert query_req_obj.ok\n",
    "    query_soup = BeautifulSoup(query_req_obj.content)\n",
    "    \n",
    "    num_nav_items = len(\n",
    "        query_soup.find_all(\"span\", class_=\"pages-nav-item\")\n",
    "    )\n",
    "    number_of_pages = num_nav_items if num_nav_items > 0 else num_nav_items + 1\n",
    "    dot_com_index = query_link.index(\".om\") + len(\".om\")\n",
    "    query_page_links = [\n",
    "        \"{}{}{}\".format(query_link[:dot_com_index+1:], \n",
    "                        \"page/{}\".format(i), \n",
    "                        query_link[dot_com_index::]) \\\n",
    "        for i in range(1, number_of_pages+1)\n",
    "    ]\n",
    "        \n",
    "    # Next, collect all of the article links that make up these pages.\n",
    "    search_page_gets = (areq.get(url) for url in query_page_links)\n",
    "    search_page_gets_results = areq.map(search_page_gets)\n",
    "    assert all([page_get.ok for page_get in search_page_gets_results])\n",
    "    \n",
    "    article_links_list = []\n",
    "    for search_page in search_page_gets_results:\n",
    "        search_page_soup = BeautifulSoup(search_page.content)\n",
    "        links_list_search = search_page_soup.find_all(\n",
    "            \"h2\", class_=\"post-title\"\n",
    "        )\n",
    "        page_article_links_list = [\n",
    "            \"https://www.ol.om{}\".format(link_soup.find(\"a\")[\"href\"]) for link_soup in links_list_search\n",
    "        ]\n",
    "        article_links_list += page_article_links_list\n",
    "    assert len(article_links_list) <= 10*number_of_pages\n",
    "    print(\n",
    "        \"Found {} articles from this search query.\".format(\n",
    "            len(article_links_list)\n",
    "        )\n",
    "    )\n",
    "    print(\"Now obtaining data for those articles.\")\n",
    "    \n",
    "    # Next, send requests to each of those links. Save the desired \n",
    "    # information for each article that was published within the last year\n",
    "    # and does in fact mention the company we're interested in (the\n",
    "    # aliases of which are listed above).\n",
    "    page_gets = (areq.get(url) for url in article_links_list)\n",
    "    page_gets_results = areq.map(page_gets)\n",
    "    \n",
    "    today_datetime = datetime.today()\n",
    "    page_dicts_list = []\n",
    "    for page in page_gets_results:\n",
    "        # Iterate over each of the page request results.\n",
    "        if page.ok:\n",
    "            # If the response to the article URL was successful.\n",
    "            page_contents = page.content\n",
    "            page_soup = BeautifulSoup(page_contents)\n",
    "            \n",
    "            # Get and verify published date.\n",
    "            article_pub_date = page_soup.find(\n",
    "                \"meta\", property=\"article:published_time\"\n",
    "            )[\"content\"]\n",
    "            article_pub_datetime = datetime.strptime(\n",
    "                article_pub_date[:10:], \"%Y-%m-%d\"\n",
    "            )\n",
    "            days_since_published = (\n",
    "                today_datetime - article_pub_datetime\n",
    "            ).days\n",
    "            if days_since_published > 365:\n",
    "                # If the article was published beyond the past year.\n",
    "                continue\n",
    "            \n",
    "            # Get and verify article text (content).\n",
    "            content_html = page_soup.find(\n",
    "                \"div\", class_=\"entry-content entry clearfix\"\n",
    "            )\n",
    "            article_content = \"\".join(\n",
    "                [p_element.text for p_element in content_html.find_all(\"p\")\n",
    "            ])\n",
    "            mentions_an_alias = any([\n",
    "                alias.lower() in article_content.lower() \\\n",
    "                for alias in entities_to_search\n",
    "            ])\n",
    "            if not mentions_an_alias:\n",
    "                # If the article does NOT mention any of the aliases that\n",
    "                # the company of interest goes by in its text.\n",
    "                continue\n",
    "            \n",
    "            # Get article title.\n",
    "            article_title = page_soup.find(\n",
    "                \"meta\", property=\"og:title\"\n",
    "            )[\"content\"]\n",
    "            \n",
    "            # Get article URL.\n",
    "            article_url = page_soup.find(\n",
    "                \"meta\", property=\"og:url\"\n",
    "            )[\"content\"]\n",
    "            \n",
    "            # Get article description\n",
    "            article_description = page_soup.find(\n",
    "                \"meta\", property=\"og:description\"\n",
    "            )[\"content\"]\n",
    "            \n",
    "            # Validate obtained data.\n",
    "            assert isinstance(article_title, str)\n",
    "            assert isinstance(article_url, str)\n",
    "            assert isinstance(article_pub_date, str)\n",
    "            assert isinstance(article_content, str)\n",
    "            assert isinstance(article_description, str)\n",
    "            \n",
    "            # Save obtained and verified data.\n",
    "            page_dict = {\"Title\": article_title,\n",
    "                         \"Article_URL\": article_url,\n",
    "                         \"Published_Date\": article_pub_date,\n",
    "                         \"Outlet\": \"ol\",\n",
    "                         \"Description\": article_description,\n",
    "                         \"Content\": article_content}\n",
    "            page_dicts_list.append(page_dict)\n",
    "    \n",
    "    # Return the final results.\n",
    "    to_return = page_dicts_list\n",
    "\n",
    "    return to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "ol_crawler(\"https://www.ol.om/?s=FAB\")[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "ol_crawler(\"https://www.ol.om/?s=%D8%A8%D9%86%D9%83+%D8%A3%D8%A8%D9%88%D8%B8%D8%A8%D9%8A+%D8%A7%D9%84%D8%A3%D9%88%D9%84\")[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alarabiya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def alarabiya_crawler(query_link: str, path_to_driver=driver_path):\n",
    "    \"\"\"\n",
    "    The purpose of this function is to provide the user with a function\n",
    "    that allows them to obtain several (if not all) of the articles that\n",
    "    are returned from a query done on the Alarabiya search feature that \n",
    "    mention the company (which can be mentioned by any of its aliases)\n",
    "    in its text.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    query_link : str\n",
    "        This string allows the user to specify the URL of the search query\n",
    "        that they performed for this outlet.\n",
    "    path_to_driver : str\n",
    "        This string allows the user to specify where on their local machine\n",
    "        a Google driver exec object can be found.\n",
    "        \n",
    "        NOTE that its default value is \"../Data/chromedriver\"\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    to_return : list of dict objects\n",
    "        This is a list of dictionaries where each dictionary corresponds\n",
    "        to an article that was found in the search query. Each dictionary\n",
    "        contains the information that was retrieved for each article.\n",
    "    \n",
    "    References\n",
    "    ----------\n",
    "    1. \n",
    "    \"\"\"\n",
    "    # First, validate the inputted data.\n",
    "    assert isinstance(query_link, str)\n",
    "    assert \"alarabiya\" in query_link\n",
    "    \n",
    "    to_return = None\n",
    "\n",
    "    return to_return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Al"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def al_ain_crawler(query_link: str, path_to_driver=driver_path):\n",
    "    \"\"\"\n",
    "    The purpose of this function is to provide the user with a function\n",
    "    that allows them to obtain several (if not all) of the articles that\n",
    "    are returned from a query done on the Al search feature that \n",
    "    mention the company (which can be mentioned by any of its aliases)\n",
    "    in its text.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    query_link : str\n",
    "        This string allows the user to specify the URL of the search query\n",
    "        that they performed for this outlet.\n",
    "    path_to_driver : str\n",
    "        This string allows the user to specify where on their local machine\n",
    "        a Google driver exec object can be found.\n",
    "        \n",
    "        NOTE that its default value is \"../Data/chromedriver\"\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    to_return : list of dict objects\n",
    "        This is a list of dictionaries where each dictionary corresponds\n",
    "        to an article that was found in the search query. Each dictionary\n",
    "        contains the information that was retrieved for each article.\n",
    "    \n",
    "    References\n",
    "    ----------\n",
    "    1.\n",
    "    \"\"\"\n",
    "    # First, validate the inputted data.\n",
    "    assert isinstance(query_link, str)\n",
    "    assert \"al\" in query_link\n",
    "    \n",
    "    to_return = None\n",
    "\n",
    "    return to_return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def sa_crawler(query_link: str, path_to_driver=driver_path):\n",
    "    \"\"\"\n",
    "    The purpose of this function is to provide the user with a function\n",
    "    that allows them to obtain several (if not all) of the articles that\n",
    "    are returned from a query done on the Sa search feature that \n",
    "    mention the company (which can be mentioned by any of its aliases)\n",
    "    in its text.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    query_link : str\n",
    "        This string allows the user to specify the URL of the search query\n",
    "        that they performed for this outlet.\n",
    "    path_to_driver : str\n",
    "        This string allows the user to specify where on their local machine\n",
    "        a Google driver exec object can be found.\n",
    "        \n",
    "        NOTE that its default value is \"../Data/chromedriver\"\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    to_return : list of dict objects\n",
    "        This is a list of dictionaries where each dictionary corresponds\n",
    "        to an article that was found in the search query. Each dictionary\n",
    "        contains the information that was retrieved for each article.\n",
    "    \n",
    "    References\n",
    "    ----------\n",
    "    1.\n",
    "    \"\"\"\n",
    "    # First, validate the inputted data.\n",
    "    assert isinstance(query_link, str)\n",
    "    assert \"sa\" in query_link\n",
    "    \n",
    "    to_return = None\n",
    "\n",
    "    return to_return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mubasher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def mubasher_crawler(query_link: str, path_to_driver=driver_path):\n",
    "    \"\"\"\n",
    "    The purpose of this function is to provide the user with a function\n",
    "    that allows them to obtain several (if not all) of the articles that\n",
    "    are returned from a query done on the Mubasher search feature that \n",
    "    mention the company (which can be mentioned by any of its aliases)\n",
    "    in its text.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    query_link : str\n",
    "        This string allows the user to specify the URL of the search query\n",
    "        that they performed for this outlet.\n",
    "    path_to_driver : str\n",
    "        This string allows the user to specify where on their local machine\n",
    "        a Google driver exec object can be found.\n",
    "        \n",
    "        NOTE that its default value is \"../Data/chromedriver\"\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    to_return : list of dict objects\n",
    "        This is a list of dictionaries where each dictionary corresponds\n",
    "        to an article that was found in the search query. Each dictionary\n",
    "        contains the information that was retrieved for each article.\n",
    "    \n",
    "    References\n",
    "    ----------\n",
    "    1.\n",
    "    \"\"\"\n",
    "    # First, validate the inputted data.\n",
    "    assert isinstance(query_link, str)\n",
    "    assert \"mubasher\" in query_link\n",
    "    \n",
    "    to_return = None\n",
    "\n",
    "    return to_return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alkhaleejonline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def alkhaleejonline_crawler(query_link: str, path_to_driver=driver_path):\n",
    "    \"\"\"\n",
    "    The purpose of this function is to provide the user with a function\n",
    "    that allows them to obtain several (if not all) of the articles that\n",
    "    are returned from a query done on the Alkhaleejonline search feature \n",
    "    that mention the company (which can be mentioned by any of its \n",
    "    aliases) in its text.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    query_link : str\n",
    "        This string allows the user to specify the URL of the search query\n",
    "        that they performed for this outlet.\n",
    "    path_to_driver : str\n",
    "        This string allows the user to specify where on their local machine\n",
    "        a Google driver exec object can be found.\n",
    "        \n",
    "        NOTE that its default value is \"../Data/chromedriver\"\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    to_return : list of dict objects\n",
    "        This is a list of dictionaries where each dictionary corresponds\n",
    "        to an article that was found in the search query. Each dictionary\n",
    "        contains the information that was retrieved for each article.\n",
    "    \n",
    "    References\n",
    "    ----------\n",
    "    1.\n",
    "    \"\"\"\n",
    "    # First, validate the inputted data.\n",
    "    assert isinstance(query_link, str)\n",
    "    assert \"alkhaleejonline\" in query_link\n",
    "    \n",
    "    to_return = None\n",
    "\n",
    "    return to_return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def aa_crawler(query_link: str, path_to_driver=driver_path):\n",
    "    \"\"\"\n",
    "    The purpose of this function is to provide the user with a function\n",
    "    that allows them to obtain several (if not all) of the articles that\n",
    "    are returned from a query done on the Aa search feature that \n",
    "    mention the company (which can be mentioned by any of its aliases)\n",
    "    in its text.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    query_link : str\n",
    "        This string allows the user to specify the URL of the search query\n",
    "        that they performed for this outlet.\n",
    "    path_to_driver : str\n",
    "        This string allows the user to specify where on their local machine\n",
    "        a Google driver exec object can be found.\n",
    "        \n",
    "        NOTE that its default value is \"../Data/chromedriver\"\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    to_return : list of dict objects\n",
    "        This is a list of dictionaries where each dictionary corresponds\n",
    "        to an article that was found in the search query. Each dictionary\n",
    "        contains the information that was retrieved for each article.\n",
    "    \n",
    "    References\n",
    "    ----------\n",
    "    1.\n",
    "    \"\"\"\n",
    "    # First, validate the inputted data.\n",
    "    assert isinstance(query_link, str)\n",
    "    assert \"aa\" in query_link\n",
    "    \n",
    "    to_return = None\n",
    "\n",
    "    return to_return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eremnews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def eremnews_crawler(query_link: str, path_to_driver=driver_path):\n",
    "    \"\"\"\n",
    "    The purpose of this function is to provide the user with a function\n",
    "    that allows them to obtain several (if not all) of the articles that\n",
    "    are returned from a query done on the Eremnews search feature that \n",
    "    mention the company (which can be mentioned by any of its aliases)\n",
    "    in its text.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    query_link : str\n",
    "        This string allows the user to specify the URL of the search query\n",
    "        that they performed for this outlet.\n",
    "    path_to_driver : str\n",
    "        This string allows the user to specify where on their local machine\n",
    "        a Google driver exec object can be found.\n",
    "        \n",
    "        NOTE that its default value is \"../Data/chromedriver\"\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    to_return : list of dict objects\n",
    "        This is a list of dictionaries where each dictionary corresponds\n",
    "        to an article that was found in the search query. Each dictionary\n",
    "        contains the information that was retrieved for each article.\n",
    "    \n",
    "    References\n",
    "    ----------\n",
    "    1. \n",
    "    \"\"\"\n",
    "    # First, validate the inputted data.\n",
    "    assert isinstance(query_link, str)\n",
    "    assert \"eremnews\" in query_link\n",
    "    \n",
    "    to_return = None\n",
    "\n",
    "    return to_return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Elnashra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def elnashra_crawler(query_link: str, path_to_driver=driver_path):\n",
    "    \"\"\"\n",
    "    The purpose of this function is to provide the user with a function\n",
    "    that allows them to obtain several (if not all) of the articles that\n",
    "    are returned from a query done on the Elnashra search feature that \n",
    "    mention the company (which can be mentioned by any of its aliases)\n",
    "    in its text.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    query_link : str\n",
    "        This string allows the user to specify the URL of the search query\n",
    "        that they performed for this outlet.\n",
    "    path_to_driver : str\n",
    "        This string allows the user to specify where on their local machine\n",
    "        a Google driver exec object can be found.\n",
    "        \n",
    "        NOTE that its default value is \"../Data/chromedriver\"\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    to_return : list of dict objects\n",
    "        This is a list of dictionaries where each dictionary corresponds\n",
    "        to an article that was found in the search query. Each dictionary\n",
    "        contains the information that was retrieved for each article.\n",
    "    \n",
    "    References\n",
    "    ----------\n",
    "    1.\n",
    "    \"\"\"\n",
    "    # First, validate the inputted data.\n",
    "    assert isinstance(query_link, str)\n",
    "    assert \"elnashra\" in query_link\n",
    "    \n",
    "    to_return = None\n",
    "\n",
    "    return to_return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aleqt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def aleqt_crawler(query_link: str, path_to_driver=driver_path):\n",
    "    \"\"\"\n",
    "    The purpose of this function is to provide the user with a function\n",
    "    that allows them to obtain several (if not all) of the articles that\n",
    "    are returned from a query done on the Aleqt search feature that \n",
    "    mention the company (which can be mentioned by any of its aliases)\n",
    "    in its text.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    query_link : str\n",
    "        This string allows the user to specify the URL of the search query\n",
    "        that they performed for this outlet.\n",
    "    path_to_driver : str\n",
    "        This string allows the user to specify where on their local machine\n",
    "        a Google driver exec object can be found.\n",
    "        \n",
    "        NOTE that its default value is \"../Data/chromedriver\"\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    to_return : list of dict objects\n",
    "        This is a list of dictionaries where each dictionary corresponds\n",
    "        to an article that was found in the search query. Each dictionary\n",
    "        contains the information that was retrieved for each article.\n",
    "    \n",
    "    References\n",
    "    ----------\n",
    "    1.\n",
    "    \"\"\"\n",
    "    # First, validate the inputted data.\n",
    "    assert isinstance(query_link, str)\n",
    "    assert \"aleqt\" in query_link\n",
    "    \n",
    "    to_return = None\n",
    "\n",
    "    return to_return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def wam_crawler(query_link: str, path_to_driver=driver_path):\n",
    "    \"\"\"\n",
    "    The purpose of this function is to provide the user with a function\n",
    "    that allows them to obtain several (if not all) of the articles that\n",
    "    are returned from a query done on the Wam search feature that \n",
    "    mention the company (which can be mentioned by any of its aliases)\n",
    "    in its text.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    query_link : str\n",
    "        This string allows the user to specify the URL of the search query\n",
    "        that they performed for this outlet.\n",
    "    path_to_driver : str\n",
    "        This string allows the user to specify where on their local machine\n",
    "        a Google driver exec object can be found.\n",
    "        \n",
    "        NOTE that its default value is \"../Data/chromedriver\"\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    to_return : list of dict objects\n",
    "        This is a list of dictionaries where each dictionary corresponds\n",
    "        to an article that was found in the search query. Each dictionary\n",
    "        contains the information that was retrieved for each article.\n",
    "    \n",
    "    References\n",
    "    ----------\n",
    "    1.\n",
    "    \"\"\"\n",
    "    # First, validate the inputted data.\n",
    "    assert isinstance(query_link, str)\n",
    "    assert \"wam\" in query_link\n",
    "    \n",
    "    to_return = None\n",
    "\n",
    "    return to_return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Youm7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def youm7_crawler(query_link: str, path_to_driver=driver_path):\n",
    "    \"\"\"\n",
    "    The purpose of this function is to provide the user with a function\n",
    "    that allows them to obtain several (if not all) of the articles that\n",
    "    are returned from a query done on the Youm7 search feature that \n",
    "    mention the company (which can be mentioned by any of its aliases)\n",
    "    in its text.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    query_link : str\n",
    "        This string allows the user to specify the URL of the search query\n",
    "        that they performed for this outlet.\n",
    "    path_to_driver : str\n",
    "        This string allows the user to specify where on their local machine\n",
    "        a Google driver exec object can be found.\n",
    "        \n",
    "        NOTE that its default value is \"../Data/chromedriver\"\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    to_return : list of dict objects\n",
    "        This is a list of dictionaries where each dictionary corresponds\n",
    "        to an article that was found in the search query. Each dictionary\n",
    "        contains the information that was retrieved for each article.\n",
    "    \n",
    "    References\n",
    "    ----------\n",
    "    1.\n",
    "    \"\"\"\n",
    "    # First, validate the inputted data.\n",
    "    assert isinstance(query_link, str)\n",
    "    assert \"youm7\" in query_link\n",
    "    \n",
    "    to_return = None\n",
    "\n",
    "    return to_return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alittihad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def alittihad_crawler(query_link: str, path_to_driver=driver_path):\n",
    "    \"\"\"\n",
    "    The purpose of this function is to provide the user with a function\n",
    "    that allows them to obtain several (if not all) of the articles that\n",
    "    are returned from a query done on the Alittihad search feature that \n",
    "    mention the company (which can be mentioned by any of its aliases)\n",
    "    in its text.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    query_link : str\n",
    "        This string allows the user to specify the URL of the search query\n",
    "        that they performed for this outlet.\n",
    "    path_to_driver : str\n",
    "        This string allows the user to specify where on their local machine\n",
    "        a Google driver exec object can be found.\n",
    "        \n",
    "        NOTE that its default value is \"../Data/chromedriver\"\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    to_return : list of dict objects\n",
    "        This is a list of dictionaries where each dictionary corresponds\n",
    "        to an article that was found in the search query. Each dictionary\n",
    "        contains the information that was retrieved for each article.\n",
    "    \n",
    "    References\n",
    "    ----------\n",
    "    1.\n",
    "    \"\"\"\n",
    "    # First, validate the inputted data.\n",
    "    assert isinstance(query_link, str)\n",
    "    assert \"alittihad\" in query_link\n",
    "    \n",
    "    to_return = None\n",
    "\n",
    "    return to_return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Amwalalghad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def amwalalghad_crawler(query_link: str, path_to_driver=driver_path):\n",
    "    \"\"\"\n",
    "    The purpose of this function is to provide the user with a function\n",
    "    that allows them to obtain several (if not all) of the articles that\n",
    "    are returned from a query done on the Amwalalghad search feature that \n",
    "    mention the company (which can be mentioned by any of its aliases)\n",
    "    in its text.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    query_link : str\n",
    "        This string allows the user to specify the URL of the search query\n",
    "        that they performed for this outlet.\n",
    "    path_to_driver : str\n",
    "        This string allows the user to specify where on their local machine\n",
    "        a Google driver exec object can be found.\n",
    "        \n",
    "        NOTE that its default value is \"../Data/chromedriver\"\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    to_return : list of dict objects\n",
    "        This is a list of dictionaries where each dictionary corresponds\n",
    "        to an article that was found in the search query. Each dictionary\n",
    "        contains the information that was retrieved for each article.\n",
    "    \n",
    "    References\n",
    "    ----------\n",
    "    1.\n",
    "    \"\"\"\n",
    "    # First, validate the inputted data.\n",
    "    assert isinstance(query_link, str)\n",
    "    assert \"amwalalghad\" in query_link\n",
    "    \n",
    "    to_return = None\n",
    "\n",
    "    return to_return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Amwal-mag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def amwal_mag_crawler(query_link: str, path_to_driver=driver_path):\n",
    "    \"\"\"\n",
    "    The purpose of this function is to provide the user with a function\n",
    "    that allows them to obtain several (if not all) of the articles that\n",
    "    are returned from a query done on the Amwal-Mag search feature that \n",
    "    mention the company (which can be mentioned by any of its aliases)\n",
    "    in its text.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    query_link : str\n",
    "        This string allows the user to specify the URL of the search query\n",
    "        that they performed for this outlet.\n",
    "    path_to_driver : str\n",
    "        This string allows the user to specify where on their local machine\n",
    "        a Google driver exec object can be found.\n",
    "        \n",
    "        NOTE that its default value is \"../Data/chromedriver\"\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    to_return : list of dict objects\n",
    "        This is a list of dictionaries where each dictionary corresponds\n",
    "        to an article that was found in the search query. Each dictionary\n",
    "        contains the information that was retrieved for each article.\n",
    "    \n",
    "    References\n",
    "    ----------\n",
    "    1.\n",
    "    \"\"\"\n",
    "    # First, validate the inputted data.\n",
    "    assert isinstance(query_link, str)\n",
    "    assert \"amwal-mag\" in query_link\n",
    "    \n",
    "    to_return = None\n",
    "\n",
    "    return to_return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full crawler function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def search_query_crawler(query_link: str, path_to_driver=driver_path):\n",
    "    \"\"\"\n",
    "    The purpose of this function is to \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    query_link : str\n",
    "        This string allows the user to specify\n",
    "    path_to_driver : str\n",
    "        This string allows the user to specify\n",
    "        \n",
    "        NOTE that its default value is \"../Data/chromedriver\"\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    to_return :\n",
    "    \n",
    "    References\n",
    "    ----------\n",
    "    1.\n",
    "    \"\"\"\n",
    "    to_return = None\n",
    "    # First, define all of the neccessary parameters that will be needed for the\n",
    "    # rest of the function.\n",
    "    allowed_outlets = [\"reuters\",\n",
    "                       \"mediain\",\n",
    "                       \"ol\",\n",
    "                       \"alarabiya\",\n",
    "                       \"al-ain\",\n",
    "                       \"sa\",\n",
    "                       \"mubasher\",\n",
    "                       \"alkhaleejonline\",\n",
    "                       \"aa\",\n",
    "                       \"eremnews\",\n",
    "                       \"elnashra\",\n",
    "                       \"aleqt\",\n",
    "                       \"wam\",\n",
    "                       \"youm7\",\n",
    "                       \"alittihad\",\n",
    "                       \"amwalalghad\",\n",
    "                       \"amwal-mag\"]\n",
    "    outlet_checker = [outlet in query_link for outlet in allowed_outlets]\n",
    "    assert sum(outlet_checker) == 1\n",
    "    outlet_used = allowed_outlets[outlet_checker.index(1)]\n",
    "    \n",
    "    # Next define everything that will be done for each outlet.\n",
    "    if outlet_used == \n",
    "    \n",
    "    return to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
